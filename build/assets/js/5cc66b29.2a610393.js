"use strict";(globalThis.webpackChunkai_humanoid_book=globalThis.webpackChunkai_humanoid_book||[]).push([[55],{1522(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"vla-integration/index","title":"Vision-Language-Action (VLA) Integration","description":"Welcome to Module 4","source":"@site/docs/vla-integration/index.md","sourceDirName":"vla-integration","slug":"/vla-integration/","permalink":"/Ai_Humanoid_Book/docs/vla-integration/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla-integration/index.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_label":"VLA Integration Home","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Nav2 for Humanoid Navigation","permalink":"/Ai_Humanoid_Book/docs/isaac-ai-brain/nav2-humanoid"},"next":{"title":"Introduction","permalink":"/Ai_Humanoid_Book/docs/vla-integration/intro"}}');var t=i(4848),s=i(8453);const a={sidebar_label:"VLA Integration Home",sidebar_position:1},r="Vision-Language-Action (VLA) Integration",l={},c=[{value:"Welcome to Module 4",id:"welcome-to-module-4",level:2},{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Module Structure",id:"module-structure",level:2},{value:"Prerequisites and Dependencies",id:"prerequisites-and-dependencies",level:2},{value:"Capstone Project",id:"capstone-project",level:2},{value:"See Also",id:"see-also",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"vision-language-action-vla-integration",children:"Vision-Language-Action (VLA) Integration"})}),"\n",(0,t.jsx)(n.h2,{id:"welcome-to-module-4",children:"Welcome to Module 4"}),"\n",(0,t.jsx)(n.p,{children:"This module explores the integration of vision, language, and action systems for robotics, enabling robots to understand, plan, and act from natural language instructions."}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Vision-Language-Action (VLA) integration represents a cutting-edge approach to robotics that combines visual perception, natural language understanding, and robotic action execution. This integration enables robots to perceive their environment, understand human commands expressed in natural language, and execute complex actions to accomplish tasks."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, students will understand:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"How speech recognition integrates with robotic systems"}),"\n",(0,t.jsx)(n.li,{children:"How Large Language Models (LLMs) can be used for cognitive planning in robotics"}),"\n",(0,t.jsx)(n.li,{children:"How perception and language systems work together"}),"\n",(0,t.jsx)(n.li,{children:"How to design end-to-end autonomous workflows"}),"\n",(0,t.jsx)(n.li,{children:"The challenges and solutions in multimodal AI systems"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"module-structure",children:"Module Structure"}),"\n",(0,t.jsx)(n.p,{children:"This module is organized into three main chapters:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"./voice-to-action",children:"Voice-to-Action pipelines"})," with speech recognition"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"./llm-planning",children:"LLM-based cognitive planning"})," mapped to ROS 2 actions"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"./vla-systems",children:"Vision-Language-Action system integration"})}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Each chapter builds upon the previous one, providing a comprehensive understanding of how these components work together to create intelligent, autonomous robotic systems."}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites-and-dependencies",children:"Prerequisites and Dependencies"}),"\n",(0,t.jsx)(n.p,{children:"Before diving into this module, students should have:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Basic understanding of robotics concepts"}),"\n",(0,t.jsxs)(n.li,{children:["Familiarity with ",(0,t.jsx)(n.a,{href:"../ros2-nervous-system/ros2-fundamentals",children:"ROS 2 fundamentals"})]}),"\n",(0,t.jsx)(n.li,{children:"Knowledge of perception systems"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of simulation environments"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"capstone-project",children:"Capstone Project"}),"\n",(0,t.jsxs)(n.p,{children:["The module concludes with a ",(0,t.jsx)(n.a,{href:"./capstone-project",children:"capstone project"})," that demonstrates an autonomous humanoid system integrating all VLA components."]}),"\n",(0,t.jsx)(n.h2,{id:"see-also",children:"See Also"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"./voice-to-action",children:"Voice-to-Action Pipelines"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"./llm-planning",children:"LLM-Based Cognitive Planning"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"./vla-systems",children:"Vision-Language-Action System Integration"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"./capstone-project",children:"Capstone: Autonomous Humanoid"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>a,x:()=>r});var o=i(6540);const t={},s=o.createContext(t);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);